



[原文链接]: http://spark.apache.org/docs/latest/api/python/#	"原文链接"

PySpark 是Apache Spark的Python接口。它不但允许你用Python API写Spark应用，而且也提供了Pyaprk Shell在分布式环境中交互式分析你的数据.PySpark支持大多数Spark功能如Spark SQL,DataFrame,Streaming,Mlib(机器学习)和Spark Core.

<img src="http://spark.apache.org/docs/latest/api/python/_images/pyspark-components.png" alt="PySpark Components" style="zoom: 50%;" />

##### Spark SQL 和 DataFrame

Spark SQL是Spark结构化数据处理模块.它提供了抽象编程的DataFrame和能充当分布式SQL查询引擎。

##### Spark 上的 pandas API 

Spark上的pandas API允许你扩大pandas数据处理工作量。通过这个包，你能够：

- 如果你已经熟悉pandas,使用Spark立即提升生产力，没有学习曲线。
- 拥有一个既适用于 Pandas（测试，较小的数据集）又适用于 Spark（分布式数据集）的代码库。
- 在pandas API和Pyspark API环境中，可以简单和没有任何负担的切换。

##### Streaming

Apache Spark 中的流功能运行在 Spark 之上，支持跨流数据和历史数据的强大交互和分析应用程序，同时继承Spark易用和容错特性。

##### MLlib

机器学习库运行在 Spark 之上，MLlib是可扩展的机器学习库，提供了一组统一的高级API,可帮助用户创建和调整实用机器学习管道。

##### Spark Core

Spark Core 是 Spark平台底层通用执行引擎，所有功能都建立在其之上，它提供了RDD（弹性分布式数据集）和内存计算能力。

